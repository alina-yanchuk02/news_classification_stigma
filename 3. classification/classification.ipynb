{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Table of contents:\n",
    "\n",
    "* [3. Classification](#chapter3)\n",
    "    * [3.1 Requirements](#section_3_1)\n",
    "    * [3.2 Imports](#section_3_2)\n",
    "    * [3.3 Get data](#section_3_3)\n",
    "    * [3.4 Train and Test dataset](#section_3_4)\n",
    "    * [3.5 Generate feature vectors](#section_3_5)\n",
    "    * [3.6 Models training and optimization (Machine Learning)](#section_3_6)\n",
    "        * [3.6.1 Logistics Regression](#section_3_6_1)\n",
    "            * [3.6.1.1 With bag-of-words](#section_3_6_1_1)\n",
    "            * [3.6.1.2 With TF-IDF](#section_3_6_1_2)\n",
    "        * [3.6.2 Linear Support Vector Classifier (SVC)](#section_3_6_2)\n",
    "            * [3.6.2.1 With bag-of-words](#section_3_6_2_1)\n",
    "            * [3.6.2.2 With TF-IDF](#section_3_6_2_2)\n",
    "        * [3.6.3 Multinomial Naive Bayes](#section_3_6_3)\n",
    "            * [3.6.3.1 With bag-of-words](#section_3_6_3_1)\n",
    "            * [3.6.3.2 With TF-IDF](#section_3_6_1_2)\n",
    "        * [3.6.4 K-Nearest Neighbors](#section_3_6_4)\n",
    "            * [3.6.4.1 With bag-of-words](#section_3_6_4_1)\n",
    "            * [3.6.4.2 With TF-IDF](#section_3_6_4_2)\n",
    "        * [3.6.5 Random Forest](#section_3_6_5)\n",
    "            * [3.6.5.1 With bag-of-words](#section_3_6_5_1)\n",
    "            * [3.6.5.2 With TF-IDF](#section_3_6_5_2)\n",
    "        * [3.6.6 XGBoost](#section_3_6_6)\n",
    "            * [3.6.6.1 With bag-of-words](#section_3_6_6_1)\n",
    "            * [3.6.6.2 With TF-IDF](#section_3_6_6_2)\n",
    "    * [3.7 Models training and optimization (Deep learning)](#section_3_7)\n",
    "        * [3.7.1 Convolutional Neural Network](#section_3_7_1)\n",
    "    * [3.8 Evaluation analysis](#section_3_8)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Classification <a class=\"anchor\" id=\"chapter3\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 Requirements <a class=\"anchor\" id=\"section_3_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xgboost in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.22.2)\n",
      "Requirement already satisfied: scipy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu "
   ]
  },
  {
   "source": [
    "## 3.2 Imports <a class=\"anchor\" id=\"section_3_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-03-24 13:15:16.251803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-03-24 13:15:16.251875: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Embedding, LSTM, Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores"
   ]
  },
  {
   "source": [
    "## 3.3 Get data <a class=\"anchor\" id=\"section_3_3\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   label                                            content\n",
       "0      0  prisão perpétua homem tentou assassinar senado...\n",
       "1      0  john nash matemático mente brilhante morre aci...\n",
       "2      1  mito reeleição mínima garantida cavaco sairá d...\n",
       "3      0  morreu rita levintalcini grande dama ciência i...\n",
       "4      0  trás porta amarela homem problemas psicológico..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>prisão perpétua homem tentou assassinar senado...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>john nash matemático mente brilhante morre aci...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>mito reeleição mínima garantida cavaco sairá d...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>morreu rita levintalcini grande dama ciência i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>trás porta amarela homem problemas psicológico...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_pickle('data_preprocessed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "source": [
    "## 3.4 Train and Test dataset <a class=\"anchor\" id=\"section_3_4\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of news in train dataset: 516\nNumber of news in train dataset: 129\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into a 80% train dataset and 20% test dataset\n",
    "\n",
    "X = data.loc[:,'content']\n",
    "y = data.loc[:,'label']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index, test_size=0.2, random_state=55)\n",
    "\n",
    "print(\"Number of news in train dataset: \" + str(len(X_train)))\n",
    "print(\"Number of news in train dataset: \" + str(len(X_test)))"
   ]
  },
  {
   "source": [
    "## 3.5 Generate feature vectors <a class=\"anchor\" id=\"section_3_5\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516, 40212)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Generate bag-of-words feature vectors\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase=False)\n",
    "bow_train = bow_vectorizer.fit_transform(X_train)\n",
    "bow_test = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "bow_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#bow_vectorizer = pickle.load(open(\"vectors/BOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516, 40212)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Generate TF-IDF feature vectors\n",
    "# \"words that are unique to particular document would have higher weights compared to words that are used commonly across documents\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False,)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "tfidf_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#tfidf_vectorizer = pickle.load(open(\"vectors/TFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(45307, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Generate Word Embeddings vectors and matrix\n",
    "# Word2Vec PT\n",
    "\n",
    "results = Counter()\n",
    "data['content'].str.lower().str.split().apply(results.update) # Number of unique words in all dataset\n",
    "\n",
    "max_length_content = 200 # Max number of content that each object/news artcile will have after padding\n",
    "vocabulary_length = len(results)\n",
    "\n",
    "# Create word index\n",
    "token = text.Tokenizer(lower=False, num_words=vocabulary_length) \n",
    "token.fit_on_texts(data['content']) # Tokenize all corpus\n",
    "word_index = token.word_index # Index of unique words (dictionary)\n",
    "\n",
    "# Convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "we_train = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_length_content)\n",
    "we_test = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_length_content)\n",
    "\n",
    "# Load the pre-trained word-embedding vectors (Word2Vec 100D)\n",
    "embedding_index = {}\n",
    "with open(\"pre-trained/cbow_s100.txt\", \"r\") as we_file:\n",
    "    first_line = True\n",
    "    for line in we_file:\n",
    "        try:\n",
    "            if first_line: \n",
    "                first_line = False\n",
    "            else:\n",
    "                line = line.split()\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                embedding_index[word] = vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Create token-embedding mapping (with words from our dataset)\n",
    "embedding_matrix = np.zeros((vocabulary_length, 100))\n",
    "for word, i in word_index.items():\n",
    "    if i > vocabulary_length - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#(Number (vocabulary size) of features/unique words in all dataset, Number of dimensions/features)\n",
    "embedding_matrix.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#embedding_index = pickle.load(open(\"vectors/WE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectorizers\n",
    "\n",
    "with open('vectors/TFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('vectors/BOW.pickle', 'wb') as f:\n",
    "    pickle.dump(bow_vectorizer, f)\n",
    "\n",
    "with open('vectors/WE.pickle', 'wb') as f:\n",
    "    pickle.dump(embedding_index, f)"
   ]
  },
  {
   "source": [
    "## 3.6 Models training and optimization (Machine Learning) <a class=\"anchor\" id=\"section_3_6\"></a>\n",
    "\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. They define how our model is actually structured.\n",
    "\n",
    "The optimization is performed with the K-fold cross validation startegy, in order to essentially combine training and validation data for both learning the model parameters and evaluating the model without introducing data leakage.\n",
    "\n",
    "Libraries used:\n",
    "\n",
    "- Scikit-optimize: uses a Sequential model-based optimization algorithm to find optimal solutions for hyperparameter search problems in less time;\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store evaluation metrics for all models\n",
    "\n",
    "evaluation_metrics = []\n",
    "\n",
    "# Function to store evaluation metrics for a model\n",
    "\n",
    "def evaluation(evaluation_metrics_list, model, model_name, test_features, test_labels, feature_vectorizer_name, deep_learning=False):\n",
    "\n",
    "    y_pred = model.predict(test_features)\n",
    "\n",
    "    if deep_learning:\n",
    "        y_pred = [int(round(p[0])) for p in y_pred]\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(test_labels, y_pred)*100\n",
    "\n",
    "    # Precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = scores(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Add metrics to evaluation list\n",
    "    evaluation_metrics_list.append(dict([\n",
    "        ('Model', model_name),\n",
    "        ('Feature Vectorizer', feature_vectorizer_name),\n",
    "        ('Accuracy (%)', round(accuracy, 2)),\n",
    "        ('Precision', round(precision, 2)),\n",
    "        ('Recall', round(recall, 2)),\n",
    "        ('F1', round(f1score, 2))\n",
    "    ]))\n",
    "\n",
    "    return evaluation_metrics_list"
   ]
  },
  {
   "source": [
    "### 3.6.1 Logistic Regression <a class=\"anchor\" id=\"section_2_6_1\"></a>\n",
    "\n",
    "- Is a classical linear method for binary classification (fits a line to best separate the two classes);\n",
    "- It can handle both dense and sparse input;\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.1.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_1_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, solver=\"liblinear\") # solver: algorithm to use in the optimization problem (liblinear for small datasets)\n",
    "#IF SAVED - Load saved model\n",
    "#model_lr_bow = pickle.load(open(\"models/LogisticRegressionBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=0.228294\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, C=best_C_lr_bow, solver=\"liblinear\").fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_bow, model_name=\"Logistic Regression\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Logistic Regression (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.1.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_1_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_tfidf = pickle.load(open(\"models/LogisticRegressionTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=10.000000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0, C=best_C_lr_tfidf, solver=\"liblinear\").fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_tfidf, model_name=\"Logistic Regression\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Logistic Regression (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 3.6.2 Linear Support Vector Classifier (SVC) <a class=\"anchor\" id=\"section_3_6_2\"></a>\n",
    "\n",
    " - Supports both dense and sparse input "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.2.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_2_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_bow = pickle.load(open(\"models/LinearSVCBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=9.299825\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0, C=best_C_svc_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_bow, model_name=\"Support Vector Classifier (Linear)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.2.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_2_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_tfidf = pickle.load(open(\"models/LinearSVCTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=3.992197\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0, C=best_C_svc_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_tfidf, model_name=\"Support Vector Classifier (Linear)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 3.6.3 Multinomial Naive Bayes <a class=\"anchor\" id=\"section_3_6_3\"></a>\n",
    "\n",
    "- Probabilistic approach to classifying documents in the case of acknowledging the frequency of a specified word in a text document;\n",
    "- Achieves well on discrete types as the number of words found in a document;\n",
    "- Conditional independence is assumed in real data and it attempts to approximate to the optimal soltuion;\n",
    "- Is a quick classifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.3.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_3_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_nb_bow = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_bow = pickle.load(open(\"models/MultinomialNaiveBayesBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=0.479317\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_bow = MultinomialNB(alpha=best_alpha_nb_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_bow, model_name=\"Naive Bayes (Multinomial)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.3.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_3_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_nb_tfidf = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_tfidf = pickle.load(open(\"models/MultinomialNaiveBayesTFIDF.pickle\", 'rb'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=0.002528\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_tfidf = MultinomialNB(alpha=best_alpha_nb_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_tfidf, model_name=\"Naive Bayes (Multinomial)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 3.6.4 K-Nearest Neighbors <a class=\"anchor\" id=\"section_3_6_4\"></a>\n",
    "\n",
    "- The data is classified based on vote among the k nearest neighbors;\n",
    "- Number of neighbors can be sqrt(number of data objects) = sqrt(516) = 22;\n",
    "- Should be preferred when the data-set is relatively small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.4.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_4_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=22, metric='euclidean') # n_neighbors = sqrt(516)\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_bow = pickle.load(open(\"models/KNNBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - N_neighbors=29 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_bow = res_gp.x[0]\n",
    "best_metric_knn_bow = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_bow, best_metric_knn_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=best_n_knn_bow, metric=best_metric_knn_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_bow, model_name=\"K-Nearest Neighbors\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.4.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_4_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=22 , metric= 'euclidean')\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_tfidf = pickle.load(open(\"models/KNNTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - N_neighbors=10 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_tfidf = res_gp.x[0]\n",
    "best_metric_knn_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_tfidf, best_metric_knn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Nearest Neighbors (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=best_n_knn_tfidf, metric=best_metric_knn_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_tfidf, model_name=\"K-Nearest Neighbors\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 3.6.5 Random Forest <a class=\"anchor\" id=\"section_3_6_5\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.5.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_5_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_bow = pickle.load(open(\"models/RandomForestBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - n_estimators=200 max_depth=200\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(10, 200, name='n_estimators'), # the number of trees in the forest\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_bow = res_gp.x[0]\n",
    "best_maxdepth_rf_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth=%d\"\"\" % (best_nestimators_rf_bow, best_maxdepth_rf_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(n_estimators=best_nestimators_rf_bow, max_depth=best_maxdepth_rf_bow, criterion='entropy', random_state=0).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_bow, model_name=\"Random Forest\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Random Forest (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.5.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_5_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_tfidf = pickle.load(open(\"models/RandomForestTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - n_estimators=174 max_depth:174\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators'),\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_tfidf, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_tfidf = res_gp.x[0]\n",
    "best_maxdepth_rf_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_tfidf, best_maxdepth_rf_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=best_nestimators_rf_tfidf, max_depth=best_maxdepth_rf_tfidf, criterion='entropy', random_state=0).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_tfidf, model_name=\"Random Forest\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Random Forest (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 3.6.6 XGBoost <a class=\"anchor\" id=\"section_2_6_6\"></a>\n",
    "\n",
    "- Has been the winning algorithm in a number of recent Kaggle competitions;\n",
    "- Is an ensemble learner like Random Forest algorithm. This means it will generate a final model based on a combination of individual models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.6.6.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_6_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "\n",
    "model_xgb_bow = XGBClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_bow = pickle.load(open(\"models/XGBoostBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_bow = res_gp.x[0]\n",
    "best_maxdepth_xgb_bow = res_gp.x[1]\n",
    "best_minchildweight_xgb_bow = res_gp.x[2]\n",
    "best_lr_xgb_bow = res_gp.x[3]\n",
    "best_gamma_xgb_bow = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_bow, best_maxdepth_xgb_bow, best_minchildweight_xgb_bow, best_lr_xgb_bow, best_gamma_xgb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[14:59:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_bow = XGBClassifier(tree_method='hist', max_depth=8, min_child_weight=3, learning_rate=0.9, gamma=1, n_estimators=1000, subsample=0.8).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_bow, model_name=\"XGBoost\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"XGBoost (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 3.6.6.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_6_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_tfidf = pickle.load(open(\"models/XGBoostTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_tfidf = res_gp.x[0]\n",
    "best_maxdepth_xgb_tfidf = res_gp.x[1]\n",
    "best_minchildweight_xgb_tfidf = res_gp.x[2]\n",
    "best_lr_xgb_tfidf = res_gp.x[3]\n",
    "best_gamma_xgb_tfidf = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_tfidf, best_maxdepth_xgb_tfidf, best_minchildweight_xgb_tfidf, best_lr_xgb_tfidf, best_gamma_xgb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15:00:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier(tree_method='hist', max_depth=8, min_child_weight=3, learning_rate=0.9, gamma=1, n_estimators=1000, subsample=0.8).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_tfidf, model_name=\"XGBoost\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"XGBoost (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "## 3.7 Models training and optimization (Deep Learning) <a class=\"anchor\" id=\"section_3_7\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.7.1 Convolutional Neural Network <a class=\"anchor\" id=\"section_3_7_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-03-24 15:00:58.535039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-24 15:00:58.535318: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-24 15:00:58.535808: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (530S): /proc/driver/nvidia/version does not exist\n",
      "2022-03-24 15:00:58.540713: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 19ms/step - loss: 0.6987 - binary_accuracy: 0.5678\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6860 - binary_accuracy: 0.5465\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6562 - binary_accuracy: 0.6105\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6657 - binary_accuracy: 0.6202\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6094 - binary_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.5344 - binary_accuracy: 0.7694\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.4385 - binary_accuracy: 0.8023\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.3885 - binary_accuracy: 0.8411\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.3000 - binary_accuracy: 0.8857\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3117 - binary_accuracy: 0.8857\n",
      "CNN (Word Embeddings) successfully trained.\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Embedding(vocabulary_length, 100, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(16, 3, padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(16, 3, padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "history = model.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model, model_name=\"Convolutional Neural Network\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"CNN (Word Embeddings) successfully trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test accuracy: 0.891\n"
     ]
    }
   ],
   "source": [
    "## Test accuracy\n",
    "\n",
    "_, test_acc = model.evaluate(we_test, y_test, verbose=0)\n",
    "\n",
    "print('Test accuracy: %.3f' % (test_acc))"
   ]
  },
  {
   "source": [
    "## 3.8 Evaluation analysis <a class=\"anchor\" id=\"section_3_8\"></a>\n",
    "\n",
    "The final evaluation (with the test dataset) is performed in the optimized models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 Model Feature Vectorizer  Accuracy  \\\n",
       "8                        Random Forest       Bag-of-words     92.25   \n",
       "9                        Random Forest             TF-IDF     91.47   \n",
       "1                  Logistic Regression             TF-IDF     90.70   \n",
       "3   Support Vector Classifier (Linear)             TF-IDF     89.92   \n",
       "4            Naive Bayes (Multinomial)       Bag-of-words     89.92   \n",
       "5            Naive Bayes (Multinomial)             TF-IDF     89.92   \n",
       "0                  Logistic Regression       Bag-of-words     86.82   \n",
       "7                  K-Nearest Neighbors             TF-IDF     86.82   \n",
       "12        Convolutional Neural Network    Word Embeddings     86.82   \n",
       "2   Support Vector Classifier (Linear)       Bag-of-words     85.27   \n",
       "11                             XGBoost             TF-IDF     82.95   \n",
       "10                             XGBoost       Bag-of-words     78.29   \n",
       "6                  K-Nearest Neighbors       Bag-of-words     75.19   \n",
       "\n",
       "    Precision  Recall    F1  \n",
       "8        0.92    0.92  0.92  \n",
       "9        0.91    0.91  0.91  \n",
       "1        0.91    0.91  0.91  \n",
       "3        0.90    0.90  0.90  \n",
       "4        0.90    0.90  0.90  \n",
       "5        0.90    0.90  0.90  \n",
       "0        0.87    0.87  0.87  \n",
       "7        0.87    0.87  0.87  \n",
       "12       0.87    0.87  0.87  \n",
       "2        0.85    0.85  0.85  \n",
       "11       0.83    0.83  0.83  \n",
       "10       0.78    0.78  0.78  \n",
       "6        0.75    0.75  0.75  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Feature Vectorizer</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>Random Forest</td>\n      <td>Bag-of-words</td>\n      <td>92.25</td>\n      <td>0.92</td>\n      <td>0.92</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Random Forest</td>\n      <td>TF-IDF</td>\n      <td>91.47</td>\n      <td>0.91</td>\n      <td>0.91</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Logistic Regression</td>\n      <td>TF-IDF</td>\n      <td>90.70</td>\n      <td>0.91</td>\n      <td>0.91</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Support Vector Classifier (Linear)</td>\n      <td>TF-IDF</td>\n      <td>89.92</td>\n      <td>0.90</td>\n      <td>0.90</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Naive Bayes (Multinomial)</td>\n      <td>Bag-of-words</td>\n      <td>89.92</td>\n      <td>0.90</td>\n      <td>0.90</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Naive Bayes (Multinomial)</td>\n      <td>TF-IDF</td>\n      <td>89.92</td>\n      <td>0.90</td>\n      <td>0.90</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>Bag-of-words</td>\n      <td>86.82</td>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>K-Nearest Neighbors</td>\n      <td>TF-IDF</td>\n      <td>86.82</td>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Convolutional Neural Network</td>\n      <td>Word Embeddings</td>\n      <td>86.82</td>\n      <td>0.87</td>\n      <td>0.87</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Support Vector Classifier (Linear)</td>\n      <td>Bag-of-words</td>\n      <td>85.27</td>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>0.85</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>XGBoost</td>\n      <td>TF-IDF</td>\n      <td>82.95</td>\n      <td>0.83</td>\n      <td>0.83</td>\n      <td>0.83</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>XGBoost</td>\n      <td>Bag-of-words</td>\n      <td>78.29</td>\n      <td>0.78</td>\n      <td>0.78</td>\n      <td>0.78</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>K-Nearest Neighbors</td>\n      <td>Bag-of-words</td>\n      <td>75.19</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(data=evaluation_metrics)\n",
    "evaluation.columns = ['Model', 'Feature Vectorizer', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "evaluation = evaluation.sort_values(by='Accuracy', ascending=False)\n",
    "evaluation"
   ]
  }
 ]
}