{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Table of contents:\n",
    "\n",
    "* [2. Classification](#chapter2)\n",
    "    * [2.1 Requirements](#section_2_1)\n",
    "    * [2.2 Imports](#section_2_2)\n",
    "    * [2.3 Get data](#section_2_3)\n",
    "    * [2.4 Train and Test dataset](#section_2_4)\n",
    "    * [2.5 Generate feature vectos](#section_2_5)\n",
    "    * [2.6 Models training and optimization](#section_2_6)\n",
    "        * [2.6.1 Logistics Regression](#section_2_6_1)\n",
    "            * [2.6.1.1 With bag-of-words](#section_2_6_1_1)\n",
    "            * [2.6.1.2 With TF-IDF](#section_2_6_1_2)\n",
    "        * [2.6.2 Linear Support Vector Classifier (SVC)](#section_2_6_2)\n",
    "            * [2.6.2.1 With bag-of-words](#section_2_6_2_1)\n",
    "            * [2.6.2.2 With TF-IDF](#section_2_6_2_2)\n",
    "        * [2.6.3 Multinomial Naive Bayes](#section_2_6_3)\n",
    "            * [2.6.3.1 With bag-of-words](#section_2_6_3_1)\n",
    "            * [2.6.3.2 With TF-IDF](#section_2_6_1_2)\n",
    "        * [2.6.4 K-Nearest Neighbors](#section_2_6_4)\n",
    "            * [2.6.4.1 With bag-of-words](#section_2_6_4_1)\n",
    "            * [2.6.4.2 With TF-IDF](#section_2_6_4_2)\n",
    "        * [2.6.5 Random Forest](#section_2_6_5)\n",
    "            * [2.6.5.1 With bag-of-words](#section_2_6_5_1)\n",
    "            * [2.6.5.2 With TF-IDF](#section_2_6_5_2)\n",
    "        * [2.6.6 XGBoost](#section_2_6_6)\n",
    "            * [2.6.6.1 With bag-of-words](#section_2_6_6_1)\n",
    "            * [2.6.6.2 With TF-IDF](#section_2_6_6_2)\n",
    "    * [2.7 Evaluation analysis](#section_2_7)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Classification <a class=\"anchor\" id=\"chapter2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1 Requirements <a class=\"anchor\" id=\"section_2_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xgboost in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.22.2)\n",
      "Requirement already satisfied: scipy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "source": [
    "## 2.2 Imports <a class=\"anchor\" id=\"section_2_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores"
   ]
  },
  {
   "source": [
    "## 2.3 Get data <a class=\"anchor\" id=\"section_2_3\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   label                                            content\n",
       "0      0  prisão perpétua homem tentou assassinar senado...\n",
       "1      0  john nash matemático mente brilhante morre aci...\n",
       "2      1  mito reeleição mínima garantida cavaco sairá d...\n",
       "3      0  morreu rita levintalcini grande dama ciência i...\n",
       "4      0  trás porta amarela homem problemas psicológico..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>prisão perpétua homem tentou assassinar senado...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>john nash matemático mente brilhante morre aci...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>mito reeleição mínima garantida cavaco sairá d...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>morreu rita levintalcini grande dama ciência i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>trás porta amarela homem problemas psicológico...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_pickle('data_preprocessed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "source": [
    "## 2.4 Train and Test dataset <a class=\"anchor\" id=\"section_2_4\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of news in train dataset: 516\nNumber of news in train dataset: 129\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into a 80% train dataset and 20% test dataset\n",
    "\n",
    "X = data.loc[:,'content']\n",
    "y = data.loc[:,'label']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index, test_size=0.2, random_state=55)\n",
    "\n",
    "print(\"Number of news in train dataset: \" + str(len(X_train)))\n",
    "print(\"Number of news in train dataset: \" + str(len(X_test)))"
   ]
  },
  {
   "source": [
    "## 2.5 Generate feature vectors <a class=\"anchor\" id=\"section_2_5\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516, 40191)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Generate bag-of-words feature vectors\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase=False)\n",
    "bow_train = bow_vectorizer.fit_transform(X_train)\n",
    "bow_test = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words)\n",
    "bow_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#bow_vectorizer = pickle.load(open(\"vectors/BOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516, 40191)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Generate TF-IDF feature vectors\n",
    "# \"words that are unique to particular document would have higher weights compared to words that are used commonly across documents\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False,)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words)\n",
    "tfidf_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#tfidf_vectorizer = pickle.load(open(\"vectors/TFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectorizers\n",
    "\n",
    "with open('vectors/TFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open('vectors/BOW.pickle', 'wb') as f:\n",
    "    pickle.dump(bow_vectorizer, f)"
   ]
  },
  {
   "source": [
    "## 2.6 Models training and optimization <a class=\"anchor\" id=\"section_2_6\"></a>\n",
    "\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. They define how our model is actually structured.\n",
    "\n",
    "The optimization is performed with the K-fold cross validation startegy, in order to essentially combine training and validation data for both learning the model parameters and evaluating the model without introducing data leakage.\n",
    "\n",
    "Libraries used:\n",
    "\n",
    "- Scikit-optimize: uses a Sequential model-based optimization algorithm to find optimal solutions for hyperparameter search problems in less time;\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store evaluation metrics for all models\n",
    "\n",
    "evaluation_metrics = []\n",
    "\n",
    "# Function to store evaluation metrics for a model\n",
    "\n",
    "def evaluation(evaluation_metrics_list, model, model_name, test_features, feature_vectorizer_name):\n",
    "\n",
    "    y_pred = model.predict(test_features)\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "    # Precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = scores(y_test, y_pred, average='micro')\n",
    "\n",
    "    # Add metrics to evaluation list\n",
    "    evaluation_metrics_list.append(dict([\n",
    "        ('Model', model_name),\n",
    "        ('Feature Vectorizer', feature_vectorizer_name),\n",
    "        ('Accuracy', round(accuracy, 2)),\n",
    "        ('Precision', round(precision, 2)),\n",
    "        ('Recall', round(recall, 2)),\n",
    "        ('F1', round(f1score, 2))\n",
    "    ]))\n",
    "\n",
    "    return evaluation_metrics_list"
   ]
  },
  {
   "source": [
    "### 2.6.1 Logistic Regression <a class=\"anchor\" id=\"section_2_6_1\"></a>\n",
    "\n",
    "- Is a classical linear method for binary classification (fits a line to best separate the two classes);\n",
    "- It can handle both dense and sparse input;\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.1.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_1_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, solver=\"liblinear\") # solver: algorithm to use in the optimization problem (liblinear for small datasets)\n",
    "#IF SAVED - Load saved model\n",
    "#model_lr_bow = pickle.load(open(\"models/LogisticRegressionBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=0.567130\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, C=best_C_lr_bow, solver=\"liblinear\").fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_bow, model_name=\"Logistic Regression\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Logistic Regression (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.1.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_1_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_tfidf = pickle.load(open(\"models/LogisticRegressionTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=8.442657\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0, C=best_C_lr_tfidf, solver=\"liblinear\").fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_tfidf, model_name=\"Logistic Regression\", test_features=tfidf_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Logistic Regression (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 2.6.2 Linear Support Vector Classifier (SVC) <a class=\"anchor\" id=\"section_2_6_2\"></a>\n",
    "\n",
    " - Supports both dense and sparse input "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.2.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_2_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_bow = pickle.load(open(\"models/LinearSVCBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=6.335602\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0, C=best_C_svc_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_bow, model_name=\"Support Vector Classifier (Linear)\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.2.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_2_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_tfidf = pickle.load(open(\"models/LinearSVCTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - C=12.264048\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0, C=best_C_svc_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_tfidf, model_name=\"Support Vector Classifier (Linear)\", test_features=tfidf_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 2.6.3 Multinomial Naive Bayes <a class=\"anchor\" id=\"section_2_6_3\"></a>\n",
    "\n",
    "- Probabilistic approach to classifying documents in the case of acknowledging the frequency of a specified word in a text document;\n",
    "- Achieves well on discrete types as the number of words found in a document;\n",
    "- Conditional independence is assumed in real data and it attempts to approximate to the optimal soltuion;\n",
    "- Is a quick classifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.3.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_3_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_nb_bow = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_bow = pickle.load(open(\"models/MultinomialNaiveBayesBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=0.567130\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_bow = MultinomialNB(alpha=best_alpha_nb_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_bow, model_name=\"Naive Bayes (Multinomial)\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.3.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_3_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_nb_tfidf = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_tfidf = pickle.load(open(\"models/MultinomialNaiveBayesTFIDF.pickle\", 'rb'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=0.006195\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_tfidf = MultinomialNB(alpha=best_alpha_nb_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_tfidf, model_name=\"Naive Bayes (Multinomial)\", test_features=tfidf_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 2.6.4 K-Nearest Neighbors <a class=\"anchor\" id=\"section_2_6_4\"></a>\n",
    "\n",
    "- The data is classified based on vote among the k nearest neighbors;\n",
    "- Number of neighbors can be sqrt(number of data objects) = sqrt(516) = 22;\n",
    "- Should be preferred when the data-set is relatively small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.4.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_4_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=22, metric='euclidean') # n_neighbors = sqrt(516)\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_bow = pickle.load(open(\"models/KNNBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - N_neighbors=28.000000 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_bow = res_gp.x[0]\n",
    "best_metric_knn_bow = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_bow, best_metric_knn_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=best_n_knn_bow, metric=best_metric_knn_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_bow, model_name=\"K-Nearest Neighbors\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.4.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_4_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=22 , metric= 'euclidean')\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_tfidf = pickle.load(open(\"models/KNNTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - N_neighbors=10.000000 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_tfidf, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_tfidf = res_gp.x[0]\n",
    "best_metric_knn_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_tfidf, best_metric_knn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Nearest Neighbors (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=best_n_knn_tfidf, metric=best_metric_knn_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_tfidf, model_name=\"K-Nearest Neighbors\", test_features=bow_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 2.6.5 Random Forest <a class=\"anchor\" id=\"section_2_6_5\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.5.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_5_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_bow = pickle.load(open(\"models/RandomForestBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=192.000000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(10, 200, name='n_estimators'), # the number of trees in the forest\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_bow = res_gp.x[0]\n",
    "best_maxdepth_rf_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth=%d\"\"\" % (best_nestimators_rf_bow, best_maxdepth_rf_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(n_estimators=best_nestimators_rf_bow, max_depth=best_maxdepth_rf_bow, criterion='entropy', random_state=0).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_bow, model_name=\"Random Forest\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Random Forest (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.5.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_5_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training (default/basic hyper-parameters)\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_tfidf = pickle.load(open(\"models/RandomForestTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: - alpha=182.000000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators',\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_tfidf, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_tfidf = res_gp.x[0]\n",
    "best_maxdepth_rf_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_tfidf, best_maxdepth_rf_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Forest (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=best_nestimators_rf_tfidf, max_depth=best_maxdepth_rf_tfidf, criterion='entropy', random_state=0).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_tfidf, model_name=\"Random Forest\", test_features=bow_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Random Forest (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "### 2.6.6 XGBoost <a class=\"anchor\" id=\"section_2_6_6\"></a>\n",
    "\n",
    "- Has been the winning algorithm in a number of recent Kaggle competitions;\n",
    "- Is an ensemble learner like Random Forest algorithm. This means it will generate a final model based on a combination of individual models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.6.6.1 With bag-of-words <a class=\"anchor\" id=\"section_2_6_6_1\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training \n",
    "\n",
    "model_xgb_bow = XGBClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_bow = pickle.load(open(\"models/XGBoostBOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_bow, bow_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_bow = res_gp.x[0]\n",
    "best_maxdepth_xgb_bow = res_gp.x[1]\n",
    "best_minchildweight_xgb_bow = res_gp.x[2]\n",
    "best_lr_xgb_bow = res_gp.x[3]\n",
    "best_gamma_xgb_bow = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_bow, best_maxdepth_xgb_bow, best_minchildweight_xgb_bow, best_lr_xgb_bow, best_gamma_xgb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[00:17:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_bow = XGBClassifier(tree_method=best_treemethod_xgb_bow, max_depth=best_maxdepth_xgb_bow, min_child_weight=best_minchildweight_xgb_bow, learning_rate=best_lr_xgb_bow, gamma=best_gamma_xgb_bow, n_estimators=1000, subsample=0.8).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_bow, model_name=\"XGBoost\", test_features=bow_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"XGBoost (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "#### 2.6.6.2 With TF-IDF <a class=\"anchor\" id=\"section_2_6_6_2\"></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[23:17:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_tfidf = pickle.load(open(\"models/XGBoostTFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_, tfidf_train, y_train, cv=3, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_tfidf = res_gp.x[0]\n",
    "best_maxdepth_xgb_tfidf = res_gp.x[1]\n",
    "best_minchildweight_xgb_tfidf = res_gp.x[2]\n",
    "best_lr_xgb_tfidf = res_gp.x[3]\n",
    "best_gamma_xgb_tfidf = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_tfidf, best_maxdepth_xgb_tfidf, best_minchildweight_xgb_tfidf, best_lr_xgb_tfidf, best_gamma_xgb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier(tree_method=best_treemethod_xgb_bow, max_depth=best_maxdepth_xgb_bow, min_child_weight=best_minchildweight_xgb_bow, learning_rate=best_lr_xgb_bow, gamma=best_gamma_xgb_bow, n_estimators=1000, subsample=0.8).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_tfidf, model_name=\"XGBoost\", test_features=bow_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"XGBoost (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "source": [
    "## 2.7 Evaluation analysis\n",
    "\n",
    "The final evaluation (with the test dataset) is performed in the optimized models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Model Feature Vectorizer  Accuracy  Precision  Recall    F1\n",
       "0  XGBoost       Bag-of-words     77.52       0.78    0.78  0.78"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Feature Vectorizer</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>XGBoost</td>\n      <td>Bag-of-words</td>\n      <td>77.52</td>\n      <td>0.78</td>\n      <td>0.78</td>\n      <td>0.78</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(data=evaluation_metrics)\n",
    "evaluation.columns = ['Model', 'Feature Vectorizer', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "evaluation = evaluation.sort_values(by='Accuracy', ascending=False)\n",
    "evaluation"
   ]
  }
 ]
}