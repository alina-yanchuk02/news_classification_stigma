{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic classification of stigmatizing mental illness articles in online news journals - April 2022\n",
    "Author: Alina Yanchuk - alinayanchuk@ua.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents:\n",
    "\n",
    "* [3. Classification](#chapter3)\n",
    "    * [3.1 Requirements](#section_3_1)\n",
    "    * [3.2 Imports](#section_3_2)\n",
    "    * [3.3 Get data](#section_3_3)\n",
    "    * [3.4 Train and Test dataset](#section_3_4)\n",
    "    * [3.5 Generate feature vectors](#section_3_5)\n",
    "    * [3.6 Models training and optimization (Machine Learning)](#section_3_6)\n",
    "        * [3.6.1 Logistics Regression](#section_3_6_1)\n",
    "            * [3.6.1.1 With bag-of-words](#section_3_6_1_1)\n",
    "            * [3.6.1.2 With TF-IDF](#section_3_6_1_2)\n",
    "            * [3.6.1.3 With LIWC](#section_3_6_1_3)\n",
    "        * [3.6.2 Linear Support Vector Classifier (SVC)](#section_3_6_2)\n",
    "            * [3.6.2.1 With bag-of-words](#section_3_6_2_1)\n",
    "            * [3.6.2.2 With TF-IDF](#section_3_6_2_2)\n",
    "            * [3.6.2.3 With LIWC](#section_3_6_2_3)\n",
    "        * [3.6.3 Multinomial Naive Bayes](#section_3_6_3)\n",
    "            * [3.6.3.1 With bag-of-words](#section_3_6_3_1)\n",
    "            * [3.6.3.2 With TF-IDF](#section_3_6_3_2)\n",
    "            * [3.6.3.3 With LIWC](#section_3_6_3_3)\n",
    "        * [3.6.4 K-Nearest Neighbors](#section_3_6_4)\n",
    "            * [3.6.4.1 With bag-of-words](#section_3_6_4_1)\n",
    "            * [3.6.4.2 With TF-IDF](#section_3_6_4_2)\n",
    "            * [3.6.4.3 With LIWC](#section_3_6_4_3)\n",
    "        * [3.6.5 Random Forest](#section_3_6_5)\n",
    "            * [3.6.5.1 With bag-of-words](#section_3_6_5_1)\n",
    "            * [3.6.5.2 With TF-IDF](#section_3_6_5_2)\n",
    "            * [3.6.5.3 With LIWC](#section_3_6_5_3)\n",
    "        * [3.6.6 XGBoost](#section_3_6_6)\n",
    "            * [3.6.6.1 With bag-of-words](#section_3_6_6_1)\n",
    "            * [3.6.6.2 With TF-IDF](#section_3_6_6_2)\n",
    "            * [3.6.6.3 With LIWC](#section_3_6_6_3)\n",
    "    * [3.7 Models training and optimization (Deep learning)](#section_3_7)\n",
    "        * [3.7.1 Convolutional Neural Network](#section_3_7_1)\n",
    "        * [3.7.2 Long Short-Term Memory (LSTM)](#section_3_7_2)\n",
    "        * [3.7.3 Bidirectional Long Short-Term Memory (Bi-LSTM)](#section_3_7_3)\n",
    "    * [3.8 Evaluation analysis](#section_3_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Requirements <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.22.2)\n",
      "Requirement already satisfied: scipy in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from xgboost) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.19.1-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 1.3 MB/s \n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pydot\n",
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 3.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from tensorflow_hub) (3.19.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages (from tensorflow_hub) (1.22.2)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Imports <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 14:31:17.332452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-09 14:31:17.332482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import floor, log\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, ,Embedding, Conv1D, Flatten, MaxPooling1D, LSTM, Bidirectional\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Get data <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>prisão perpétua homem tentou assassinar senado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>john nash matemático mente brilhante morre aci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mito reeleição mínima garantida cavaco sairá d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>morreu rita levintalcini grande dama ciência i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>trás porta amarela homem problemas psicológico...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content\n",
       "0      0  prisão perpétua homem tentou assassinar senado...\n",
       "1      0  john nash matemático mente brilhante morre aci...\n",
       "2      1  mito reeleição mínima garantida cavaco sairá d...\n",
       "3      0  morreu rita levintalcini grande dama ciência i...\n",
       "4      0  trás porta amarela homem problemas psicológico..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data_preprocessed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Train and Test dataset <a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news in train dataset: 782\n",
      "Number of news in train dataset: 196\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into a 80% train dataset and 20% test dataset\n",
    "\n",
    "X = data.loc[:,'content']\n",
    "y = data.loc[:,'label']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index, test_size=0.2, random_state=55)\n",
    "\n",
    "print(\"Number of news in train dataset: \" + str(len(X_train)))\n",
    "print(\"Number of news in train dataset: \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Generate feature vectors <a class=\"anchor\" id=\"section_3_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 44544)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate bag-of-words feature vectors\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase=False)\n",
    "bow_train = bow_vectorizer.fit_transform(X_train)\n",
    "bow_test = bow_vectorizer.transform(X_test)\n",
    "\n",
    "bow_train = (bow_train - np.min(bow_train)) / (np.max(bow_train) - np.min(bow_train)) # Normalize to range [0, 1]\n",
    "bow_test = (bow_test - np.min(bow_test)) / (np.max(bow_test) - np.min(bow_test))\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "bow_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#bow_vectorizer = pickle.load(open(\"vectors/BOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 44544)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TF-IDF feature vectors\n",
    "# \"words that are unique to particular document would have higher weights compared to words that are used commonly across documents\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False,)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "tfidf_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#tfidf_vectorizer = pickle.load(open(\"vectors/TFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 464)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate LIWC-PT feature vectors\n",
    "# \"lexicon that assigns words to categories\"\n",
    "# http://143.107.183.175:21380/portlex/index.php/pt/projetos/liwc\n",
    "# https://lit.eecs.umich.edu/geoliwc/liwc_dictionary.html\n",
    "\n",
    "liwc_dictionary = {}\n",
    "# IF SAVED - Load saved mapping\n",
    "#liwc_dictionary = pickle.load(open(\"vectors/LIWC.pickle\", 'rb'))\n",
    "\n",
    "\n",
    "# Read the file and create the LIWC dictionary mapping (word-categories) (If not in repo, need to download from official website above)\n",
    "with open(\"dictionary/liwc.txt\", \"r\", encoding = \"ISO-8859-1\") as liwc_file:\n",
    "    start = False\n",
    "    for line in liwc_file:\n",
    "        try:\n",
    "            line = line.split()\n",
    "            if start==False:\n",
    "                if line[0]==\"a\":\n",
    "                    start=True\n",
    "                    word = line[0]\n",
    "                    vector = np.asarray(line[1:], dtype='float32')\n",
    "                    liwc_dictionary[word] = vector\n",
    "            else:\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                liwc_dictionary[word] = vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "liwc_file.close()\n",
    "\n",
    "# Create the feature vectors: each vector will have all the LIWC categories and the corresponding values will be incremented everytime a word in the news article (document) fits in that category\n",
    "\n",
    "documents_train = list(X_train.values.flatten()) # List of all news\n",
    "documents_train = [string.split() for string in documents_train] # Split each news article in tokens/words\n",
    "\n",
    "liwc_train = np.zeros((len(X_train), 464))\n",
    "for document in documents_train:\n",
    "    i = documents_train.index(document)\n",
    "    for word in document:\n",
    "        if word in liwc_dictionary:\n",
    "            vector = liwc_dictionary[word]\n",
    "            for index in vector:\n",
    "                liwc_train[i][int(index)-1] = liwc_train[i][int(index)-1] + 1\n",
    "\n",
    "documents_test = list(X_test.values.flatten())\n",
    "documents_test = [string.split() for string in documents_test]\n",
    "\n",
    "liwc_test = np.zeros((len(X_test), 464))\n",
    "for document in documents_test:\n",
    "    i = documents_test.index(document)\n",
    "    for word in document:\n",
    "        if word in liwc_dictionary:\n",
    "            vector = liwc_dictionary[word]\n",
    "            for index in vector:\n",
    "                liwc_test[i][int(index)-1] = liwc_test[i][int(index)-1] + 1\n",
    "\n",
    "liwc_train = (liwc_train - np.min(liwc_train)) / (np.max(liwc_train) - np.min(liwc_train)) # Normalize to range [0, 1]\n",
    "liwc_test = (liwc_test - np.min(liwc_test)) / (np.max(liwc_test) - np.min(liwc_test)) \n",
    "\n",
    "# (Number of news, Number of features/categories in training dataset)\n",
    "liwc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51304, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Word Embeddings vectors and matrix\n",
    "# Glove PT 300D from NILC-Embeddings \n",
    "# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
    "\n",
    "words_per_article = list(map(lambda x : len(x.split()),data.content))\n",
    "avg = sum(words_per_article)/len(words_per_article) # Average number of words per news article\n",
    "avg = pow(2, floor(log(avg)/log(2))) # Convert to nearest power of 2\n",
    "\n",
    "results = Counter()\n",
    "data['content'].str.lower().str.split().apply(results.update) # Number of unique words in all dataset\n",
    "\n",
    "max_length_content = int(avg) # Max number of content that each object/news article will have after padding\n",
    "vocabulary_length = len(results) # Max number of unique words\n",
    "\n",
    "# Create word index\n",
    "token = text.Tokenizer(lower=False, num_words=vocabulary_length) \n",
    "token.fit_on_texts(data['content']) # Tokenize all corpus\n",
    "word_index = token.word_index # Index of unique words (dictionary)\n",
    "\n",
    "# Convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "we_train = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_length_content)\n",
    "we_test = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_length_content)\n",
    "\n",
    "# Load the pre-trained word-embedding vectors (Glove 300D)\n",
    "embedding_index = {}\n",
    "# IF SAVED - Load saved mapping\n",
    "#embedding_index = pickle.load(open(\"vectors/WE_glove.pickle\", 'rb'))\n",
    "\n",
    "# If not in repo, need to download from official website above\n",
    "with open(\"pre-trained/glove_s300.txt\", \"r\") as we_file:\n",
    "    first_line = True\n",
    "    for line in we_file:\n",
    "        try:\n",
    "            if first_line: # Ignore header\n",
    "                first_line = False\n",
    "            else:\n",
    "                line = line.split()\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                embedding_index[word] = vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Create token-embedding mapping (with words from our dataset)\n",
    "embedding_matrix = np.zeros((vocabulary_length, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > vocabulary_length - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#(Number (vocabulary size) of features/unique words in all dataset, Number of dimensions/features)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature vectorizers and mappings\n",
    "\n",
    "\n",
    "with open('vectors/TFIDF.pickle', 'wb') as f1:\n",
    "    pickle.dump(tfidf_vectorizer, f1)\n",
    "f1.close()\n",
    "\n",
    "with open('vectors/BOW.pickle', 'wb') as f2:\n",
    "    pickle.dump(bow_vectorizer, f2)\n",
    "f2.close()\n",
    "\n",
    "with open('vectors/WE.pickle', 'wb') as f3:\n",
    "    pickle.dump(embedding_index, f3)\n",
    "f3.close()\n",
    "\n",
    "with open('vectors/LIWC.pickle', 'wb') as f4:\n",
    "    pickle.dump(liwc_dictionary, f4)\n",
    "f4.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Models training and optimization (Machine Learning) <a class=\"anchor\" id=\"section_3_6\"></a>\n",
    "\n",
    "#### Training\n",
    "\n",
    "Attempt to get the best combination of weights and bias to the model to minimize the loss function over the prediction range and incrementally improve model's ability to predict the classes.\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. They define how our model is actually structured. Hyper-parameter tuning is just an optimization loop on top of ML model learning to find the set of hyper-parameters leading to the lowest error on the validation set.\n",
    "\n",
    "-  The optimization for the classic Machine Learning algorithms is performed with the K-fold cross (K=5) validation strategy, in order to essentially combine training and validation data for both learning the model parameters and evaluating the model without introducing data leakage.\n",
    "\n",
    "    Library used -> Scikit-optimize: uses a Sequential model-based optimization algorithm to find optimal solutions for hyperparameter search problems in less time. \n",
    "    \n",
    "    Reference: https://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html\n",
    "\n",
    "- Optimization for the Deep Learning algorithms:\n",
    "\n",
    "    Library used -> Keras Tuner: The Hyperband algorithm is a variation of random search, but with some explore-exploit theory to find the best time allocation for each of the configurations.\n",
    "    \n",
    "    Reference: https://www.tensorflow.org/tutorials/keras/keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store evaluation metrics for all models\n",
    "\n",
    "evaluation_metrics = []\n",
    "\n",
    "# Function to store evaluation metrics for a model\n",
    "\n",
    "def evaluation(evaluation_metrics_list, model, model_name, test_features, test_labels, feature_vectorizer_name, deep_learning=False):\n",
    "\n",
    "    y_pred = model.predict(test_features)\n",
    "\n",
    "    if deep_learning:\n",
    "        y_pred = [int(round(p[0])) for p in y_pred]\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(test_labels, y_pred)*100\n",
    "\n",
    "    # Precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = scores(y_test, y_pred, average='binary')\n",
    "\n",
    "    # Add metrics to evaluation list\n",
    "    evaluation_metrics_list.append(dict([\n",
    "        ('Model', model_name),\n",
    "        ('Feature Vectorizer', feature_vectorizer_name),\n",
    "        ('Accuracy (%)', round(accuracy, 2)),\n",
    "        ('Precision', round(precision, 3)),\n",
    "        ('Recall', round(recall, 3)),\n",
    "        ('F1', round(f1score, 3))\n",
    "    ]))\n",
    "\n",
    "    return evaluation_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict labels (using the trained model) to a set of received features\n",
    "\n",
    "def prediction(trained_model, features):\n",
    "\n",
    "    y_pred = trained_model.predict(features)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert predicted labels to original data's csv file\n",
    "\n",
    "def insert(pred_labels, file_name):\n",
    "\n",
    "    original_data = pd.read_pickle(file_name)\n",
    "\n",
    "    original_data.insert(9, 'Predicted label', pred_labels)\n",
    "\n",
    "    original_data.to_csv(\"result_\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Logistic Regression <a class=\"anchor\" id=\"section_2_6_1\"></a>\n",
    "\n",
    "- Is a classical linear method for binary classification (fits a line to best separate the two classes);\n",
    "- It can handle both dense and sparse input;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=10.000000\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0) # solver: algorithm to use in the optimization problem (liblinear for small datasets)\n",
    "#IF SAVED - Load saved model\n",
    "#model_lr_bow = pickle.load(open(\"models/LogisticRegressionBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, C=best_C_lr_bow, solver=\"liblinear\").fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_bow, model_name=\"Logistic Regression\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Logistic Regression (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=8.442657\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_tfidf = pickle.load(open(\"models/LogisticRegressionTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0, C=best_C_lr_tfidf, solver=\"liblinear\").fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_tfidf, model_name=\"Logistic Regression\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Logistic Regression (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.3 With LIWC <a class=\"anchor\" id=\"section_3_6_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=10.000000\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_liwc = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_LIWC = pickle.load(open(\"models/LogisticRegressionLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_liwc = LogisticRegression(random_state=0, C=best_C_lr_liwc, solver=\"liblinear\").fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_liwc, model_name=\"Logistic Regression\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Logistic Regression (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Linear Support Vector Classifier (SVC) <a class=\"anchor\" id=\"section_3_6_2\"></a>\n",
    "\n",
    " - Supports both dense and sparse input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=3.461427\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_bow = pickle.load(open(\"models/LinearSVCBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0, C=best_C_svc_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_bow, model_name=\"Support Vector Classifier (Linear)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=9.299825\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_tfidf = pickle.load(open(\"models/LinearSVCTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0, C=best_C_svc_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_tfidf, model_name=\"Support Vector Classifier (Linear)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.4 With LIWC <a class=\"anchor\" id=\"section_3_6_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=15.000000\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_liwc = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_liwc = pickle.load(open(\"models/LinearSVCLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Classifier (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_liwc = LinearSVC(random_state=0, C=best_C_svc_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_liwc, model_name=\"Support Vector Classifier (Linear)\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 Multinomial Naive Bayes <a class=\"anchor\" id=\"section_3_6_3\"></a>\n",
    "\n",
    "- Probabilistic approach to classifying documents in the case of acknowledging the frequency of a specified word in a text document;\n",
    "- Achieves well on discrete types as the number of words found in a document;\n",
    "- Conditional independence is assumed in real data and it attempts to approximate to the optimal soltuion;\n",
    "- Is a quick classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.000014\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_bow = MultinomialNB(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_bow = pickle.load(open(\"models/MultinomialNaiveBayesBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_bow = MultinomialNB(alpha=best_alpha_nb_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_bow, model_name=\"Naive Bayes (Multinomial)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.238853\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_tfidf = MultinomialNB(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_tfidf = pickle.load(open(\"models/MultinomialNaiveBayesTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_tfidf = MultinomialNB(alpha=best_alpha_nb_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_tfidf, model_name=\"Naive Bayes (Multinomial)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.3 With LIWC <a class=\"anchor\" id=\"section_3_6_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_liwc = MultinomialNB(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_liwc = pickle.load(open(\"models/MultinomialNaiveBayesLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_liwc = MultinomialNB(alpha=best_alpha_nb_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_liwc, model_name=\"Naive Bayes (Multinomial)\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.4 K-Nearest Neighbors <a class=\"anchor\" id=\"section_3_6_4\"></a>\n",
    "\n",
    "- The data is classified based on vote among the k nearest neighbors;\n",
    "- Number of neighbors can be sqrt(number of data objects) = int(sqrt(978)) = 31;\n",
    "- Should be preferred when the data-set is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=1 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(random_state=0) \n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_bow = pickle.load(open(\"models/KNNBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_bow = res_gp.x[0]\n",
    "best_metric_knn_bow = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_bow, best_metric_knn_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=best_n_knn_bow, metric=best_metric_knn_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_bow, model_name=\"K-Nearest Neighbors\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=11 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_tfidf = pickle.load(open(\"models/KNNTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_tfidf = res_gp.x[0]\n",
    "best_metric_knn_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_tfidf, best_metric_knn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=best_n_knn_tfidf, metric=best_metric_knn_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_tfidf, model_name=\"K-Nearest Neighbors\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.2 With LIWC <a class=\"anchor\" id=\"section_3_6_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=1 Metric= manhattan\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_liwc = KNeighborsClassifier(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_liwc = pickle.load(open(\"models/KNNLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 30, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_liwc = res_gp.x[0]\n",
    "best_metric_knn_liwc = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_liwc, best_metric_knn_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (LIWC) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_liwc = KNeighborsClassifier(n_neighbors=best_n_knn_liwc, metric=best_metric_knn_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_liwc, model_name=\"K-Nearest Neighbors\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (LIWC) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.5 Random Forest <a class=\"anchor\" id=\"section_3_6_5\"></a>\n",
    "\n",
    "- Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=158 max_depth=30\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_bow = pickle.load(open(\"models/RandomForestBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(10, 200, name='n_estimators'), # the number of trees in the forest\n",
    "          Integer(3,100, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_bow = res_gp.x[0]\n",
    "best_maxdepth_rf_bow = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth=%d\"\"\" % (best_nestimators_rf_bow, best_maxdepth_rf_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(n_estimators=best_nestimators_rf_bow, max_depth=best_maxdepth_rf_bow, criterion='entropy', random_state=0).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_bow, model_name=\"Random Forest\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Random Forest (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_5_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=204 max_depth:28\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_tfidf = pickle.load(open(\"models/RandomForestTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators'),\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_tfidf = res_gp.x[0]\n",
    "best_maxdepth_rf_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_tfidf, best_maxdepth_rf_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=best_nestimators_rf_tfidf, max_depth=best_maxdepth_rf_tfidf, criterion='entropy', random_state=0).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_tfidf, model_name=\"Random Forest\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Random Forest (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.3 With LIWC <a class=\"anchor\" id=\"section_3_6_5_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=101 max_depth:19\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_liwc = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_liwc = pickle.load(open(\"models/RandomForestLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators'),\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_liwc = res_gp.x[0]\n",
    "best_maxdepth_rf_liwc = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_liwc, best_maxdepth_rf_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (LIWC) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_liwc = RandomForestClassifier(n_estimators=best_nestimators_rf_liwc, max_depth=best_maxdepth_rf_liwc, criterion='entropy', random_state=0).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_liwc, model_name=\"Random Forest\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Random Forest (LIWC) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.6 XGBoost <a class=\"anchor\" id=\"section_2_6_6\"></a>\n",
    "\n",
    "- Has been the winning algorithm in a number of recent Kaggle competitions;\n",
    "- Is an ensemble learner like Random Forest algorithm. This means it will generate a final model based on a combination of individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.6.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_6_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model_xgb_bow = XGBClassifier(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_bow = pickle.load(open(\"models/XGBoostBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_bow = res_gp.x[0]\n",
    "best_maxdepth_xgb_bow = res_gp.x[1]\n",
    "best_minchildweight_xgb_bow = res_gp.x[2]\n",
    "best_lr_xgb_bow = res_gp.x[3]\n",
    "best_gamma_xgb_bow = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_bow, best_maxdepth_xgb_bow, best_minchildweight_xgb_bow, best_lr_xgb_bow, best_gamma_xgb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_bow = XGBClassifier(tree_method='hist', max_depth=8, min_child_weight=3, learning_rate=0.9, gamma=1, n_estimators=1000, subsample=0.8).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_bow, model_name=\"XGBoost\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"XGBoost (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.6.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_6_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_tfidf = pickle.load(open(\"models/XGBoostTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_tfidf = res_gp.x[0]\n",
    "best_maxdepth_xgb_tfidf = res_gp.x[1]\n",
    "best_minchildweight_xgb_tfidf = res_gp.x[2]\n",
    "best_lr_xgb_tfidf = res_gp.x[3]\n",
    "best_gamma_xgb_tfidf = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_tfidf, best_maxdepth_xgb_tfidf, best_minchildweight_xgb_tfidf, best_lr_xgb_tfidf, best_gamma_xgb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:27:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_tfidf = XGBClassifier(tree_method='hist', max_depth=8, min_child_weight=3, learning_rate=0.9, gamma=1, n_estimators=1000, subsample=0.8).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_tfidf, model_name=\"XGBoost\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"XGBoost (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.6.3 With LIWC <a class=\"anchor\" id=\"section_3_6_6_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model_xgb_liwc = XGBClassifier(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_xgb_liwc = pickle.load(open(\"models/XGBoostLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Categorical(['exact', 'approx', 'hist'], name='tree_method'),  # tree method to use\n",
    "          Integer(3, 10, name ='max_depth'), # the maximum allowable depth for each decision tree \n",
    "          Real(1, 6, name ='min_child_weight'),\n",
    "          Real(0, 1, name='learning_rate'), # boosting learning rate\n",
    "          Real(0, 9, name=\"gamma\") # minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "          ] \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_xgb_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_xgb_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_treemethod_xgb_liwc = res_gp.x[0]\n",
    "best_maxdepth_xgb_liwc = res_gp.x[1]\n",
    "best_minchildweight_xgb_liwc = res_gp.x[2]\n",
    "best_lr_xgb_liwc = res_gp.x[3]\n",
    "best_gamma_xgb_liwc = res_gp.x[4]\n",
    "\n",
    "print(\"\"\"Best parameters: - tree_method=%s max_depth=%d min_child_weight=%f learning_rate=%f gamma=%f\"\"\" % (best_treemethod_xgb_liwc, best_maxdepth_xgb_liwc, best_minchildweight_xgb_liwc, best_lr_xgb_liwc, best_gamma_xgb_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:30:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost (LIWC) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_xgb_liwc = XGBClassifier(tree_method='hist', max_depth=8, min_child_weight=3, learning_rate=0.9, gamma=1, n_estimators=1000, subsample=0.8).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/XGBoostLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_xgb_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_xgb_liwc, model_name=\"XGBoost\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"XGBoost (LIWC) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Models training and optimization (Deep Learning) <a class=\"anchor\" id=\"section_3_7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Convolutional Neural Network <a class=\"anchor\" id=\"section_3_7_1\"></a>\n",
    "\n",
    "- Multi-layered artificial neural networks with the ability to detect complex features in data;\n",
    "- Have been showing good results on text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_cnn_model_for_tuning(hp):\n",
    "    model_cnn_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_cnn_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    hp_rates = hp.Choice('rate', values=[0.2,0.3,0.4,0.5])\n",
    "    model_cnn_we.add(Dropout(rate=hp_rates))\n",
    "    hp_filters = hp.Int('filters', min_value = 10, max_value = 60)\n",
    "    hp_kernels = hp.Int('kernel_size', min_value = 3, max_value = 15)\n",
    "    model_cnn_we.add(Conv1D(filters=hp_filters, kernel_size=hp_kernels, padding=\"valid\", activation=\"relu\"))\n",
    "    model_cnn_we.add(MaxPooling1D())\n",
    "    model_cnn_we.add(Conv1D(filters=hp_filters, kernel_size=hp_kernels, padding=\"valid\", activation=\"relu\"))\n",
    "    model_cnn_we.add(MaxPooling1D())\n",
    "    model_cnn_we.add(Flatten())\n",
    "    hp_units = hp.Int('units', min_value = 10, max_value = 100, step=10)\n",
    "    model_cnn_we.add(Dense(units=hp_units, activation=\"relu\"))\n",
    "    model_cnn_we.add(Dropout(rate=hp_rates))\n",
    "    model_cnn_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_cnn_we.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    return model_cnn_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_cnn_we = pickle.load(open(\"models/CNNWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 22s]\n",
      "val_accuracy: 0.9299362897872925\n",
      "\n",
      "Best val_accuracy So Far: 0.9299362897872925\n",
      "Total elapsed time: 00h 04m 39s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 512, 300)          15391200  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512, 300)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 500, 15)           58515     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 250, 15)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 238, 15)           2940      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 119, 15)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1785)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                160740    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 90)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 91        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,613,486\n",
      "Trainable params: 222,286\n",
      "Non-trainable params: 15,391,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_cnn_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='cnn')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_cnn_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_cnn_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 9s 295ms/step - loss: 0.6981 - accuracy: 0.5716\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.3868 - accuracy: 0.8593\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 7s 286ms/step - loss: 0.2440 - accuracy: 0.9182\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 0.2139 - accuracy: 0.9156\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 0.1835 - accuracy: 0.9335\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.1491 - accuracy: 0.9412\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 7s 288ms/step - loss: 0.1685 - accuracy: 0.9399\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0943 - accuracy: 0.9744\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0630 - accuracy: 0.9795\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0502 - accuracy: 0.9872\n",
      "2022-04-06 22:37:32.420713: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: ram://38c0d39d-9d60-46f3-95ca-5a364f8b0f83/assets\n",
      "CNN (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_cnn_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "#plot_model(model_cnn_we, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Model saving\n",
    "with open('models/CNNWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_cnn_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_cnn_we, model_name=\"Convolutional Neural Network\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"CNN (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Long Short-Term memory (LSTM)<a class=\"anchor\" id=\"section_3_7_2\"></a>\n",
    "\n",
    "- Type of recurrent neural network that is better in terms of memory;\n",
    "- Holds the required information and discard the information which is not required or useful for further prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_lstm_model_for_tuning(hp):\n",
    "    model_lstm_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_lstm_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    model_lstm_we.add(LSTM(units=hp.Int('units',min_value=32, max_value=512, step=32), activation=\"sigmoid\"))\n",
    "    model_lstm_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_lstm_we.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model_lstm_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_lstm_we = pickle.load(open(\"models/LSTMWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 08m 05s]\n",
      "val_accuracy: 0.7579618096351624\n",
      "\n",
      "Best val_accuracy So Far: 0.8216560482978821\n",
      "Total elapsed time: 00h 25m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 512, 300)          15391200  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 192)               378624    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 193       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,770,017\n",
      "Trainable params: 378,817\n",
      "Non-trainable params: 15,391,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_lstm_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='lstm')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_lstm_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_lstm_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 49s 2s/step - loss: 0.6631 - accuracy: 0.5934\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.5696 - accuracy: 0.7187\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.4837 - accuracy: 0.7839\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 46s 2s/step - loss: 0.3930 - accuracy: 0.8248\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.3891 - accuracy: 0.8171\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.3252 - accuracy: 0.8619\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.2722 - accuracy: 0.8926\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 47s 2s/step - loss: 0.2166 - accuracy: 0.9118\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 46s 2s/step - loss: 0.2437 - accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 45s 2s/step - loss: 0.2021 - accuracy: 0.9258\n",
      "INFO:tensorflow:Assets written to: ram://f67ac756-5893-41b7-8e6d-6227af0c0fa2/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f7833c1d850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "LSTM (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_lstm_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LSTMWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_lstm_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_lstm_we, model_name=\"Long short-term memory (LSTM)\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"LSTM (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 Bidirectional Long short-term memory (Bi-LSTM) <a class=\"anchor\" id=\"section_3_7_3\"></a>\n",
    "\n",
    "- Bidirection: propagates the input forward and backwards through the RNN layer and then concatenates the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_bilstm_model_for_tuning(hp):\n",
    "    model_bilstm_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_bilstm_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    model_bilstm_we.add(Bidirectional(LSTM(units=hp.Int('units',min_value=32, max_value=512, step=32), activation=\"sigmoid\")))\n",
    "    model_bilstm_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_bilstm_we.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model_bilstm_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_bilstm_we = pickle.load(open(\"models/BILSTMWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 10s]\n",
      "val_accuracy: 0.7133758068084717\n",
      "\n",
      "Best val_accuracy So Far: 0.7834395170211792\n",
      "Total elapsed time: 00h 38m 12s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 512, 300)          15391200  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 960)              2999040   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 961       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,391,201\n",
      "Trainable params: 3,000,001\n",
      "Non-trainable params: 15,391,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_bilstm_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='bilstm')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_bilstm_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_bilstm_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 279s 11s/step - loss: 0.7733 - accuracy: 0.5371\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 272s 11s/step - loss: 0.6064 - accuracy: 0.6726\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 274s 11s/step - loss: 0.5480 - accuracy: 0.7289\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 274s 11s/step - loss: 0.4756 - accuracy: 0.7916\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 270s 11s/step - loss: 0.3800 - accuracy: 0.8350\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 271s 11s/step - loss: 0.3691 - accuracy: 0.8389\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 270s 11s/step - loss: 0.3179 - accuracy: 0.8581\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 269s 11s/step - loss: 0.2651 - accuracy: 0.8887\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 270s 11s/step - loss: 0.2453 - accuracy: 0.8913\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 270s 11s/step - loss: 0.1830 - accuracy: 0.9297\n",
      "INFO:tensorflow:Assets written to: ram://c5baa2c6-3f2f-42b3-babf-d2737eb2eb53/assets\n",
      "INFO:tensorflow:Assets written to: ram://c5baa2c6-3f2f-42b3-babf-d2737eb2eb53/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f782d7671c0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f782d77bbe0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "Bi-LSTM (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_bilstm_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# Model saving\n",
    "with open('models/BILSTMWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_bilstm_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_bilstm_we, model_name=\"Bidirectional Long short-term memory (Bi-LSTM)\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"Bi-LSTM (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Evaluation analysis <a class=\"anchor\" id=\"section_3_8\"></a>\n",
    "\n",
    "The final evaluation (with the test dataset) is performed in the optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>91.84</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Feature Vectorizer  Accuracy  Precision  Recall    F1\n",
       "0  Logistic Regression       Bag-of-words     91.84       0.92    0.92  0.92"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(data=evaluation_metrics)\n",
    "evaluation.columns = ['Model', 'Feature Vectorizer', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "evaluation = evaluation.sort_values(by='Accuracy', ascending=False)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
